\relax 
\citation{juliani2020}
\citation{narvekar_learning_2018}
\@writefile{toc}{\contentsline {title}{Solving an agility maze using Reinforcement Learning with Unity ML-Agents and Curriculum Learning}{1}{}\protected@file@percent }
\@writefile{toc}{\authcount {1}}
\@writefile{toc}{\contentsline {author}{\textbf  {Mario da Graca}}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{}\protected@file@percent }
\newlabel{sec:introduction}{{1}{1}}
\citation{sutton_reinforcement_2018}
\citation{sutton_reinforcement_2018}
\@writefile{toc}{\contentsline {section}{\numberline {2}Basics}{2}{}\protected@file@percent }
\newlabel{sec:basics}{{2}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Reinforcement Learning}{2}{}\protected@file@percent }
\newlabel{subsec:reinforcement-learning}{{2.1}{2}}
\citation{schulman_proximal_2017}
\citation{konda_onactor-critic_2003}
\citation{Haas2014AHO}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Proximal Policy Optmization (PPO)}{3}{}\protected@file@percent }
\newlabel{subsec:proximal-policy-optmization-(ppo)}{{2.2}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Unity and ML-Agents}{3}{}\protected@file@percent }
\newlabel{subsec:unity-and-ml-agents}{{2.3}{3}}
\bibstyle{splncs04}
\bibdata{references}
\bibcite{Haas2014AHO}{1}
\bibcite{juliani2020}{2}
\bibcite{konda_onactor-critic_2003}{3}
\bibcite{narvekar_learning_2018}{4}
\bibcite{schulman_proximal_2017}{5}
\bibcite{sutton_reinforcement_2018}{6}
\gdef \@abspage@last{4}
