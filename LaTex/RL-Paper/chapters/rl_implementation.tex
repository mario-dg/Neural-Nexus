%! Author = mario
%! Date = 30.01.2023


\section{RL implementation}\label{sec:rl-implementation}

\subsection{Unity ML-Agents}\label{subsec:unity-ml-agents}
The Unity ML-Agents Toolkit is an open-source plugin for the Unity game engine.
It bundles easy to use functionality for environment creation, agent training, state-of-the-art RL algorithms and metric logging
all via the given Python API and Unity's editor interface~\cite{juliani2020}.
When working with this package there are only three main entities working together to build the training pipeline: Sensors, Agents and the Academy.
The Agent is the main component, it indicates that a game object serves as a trainable agent in the environment.
It can collect observations via different types of Sensors, take actions chosen by the policy and collect rewards to further improve its performance.
Possible types of observations are vectors of any length and any numerical datatype, images or ray-cast results.
The available RayPerceptionSensor casts rays from a specified point in any direction with an arbitrary length to determine distances to nearby game objects.
And lastly the Academy serves as the connection point, as it manages the simulation, its global steps and the agents in the environment.
The Academy can also be used to speed up the training process, by multiplying the training arenas in the scene, allowing a parallel training of agents on the same policy.
An experience buffer of specified size is filled up by all agents and once it's full the policy is updated.

\subsection{Basics}\label{subsec:basics}
The reason of this project is to compare the default reinforcement learning approach with the idea of curriculum learning.
Very hard or enduring tasks might take a long time for an agent to solve, hence the idea of splitting it into smaller subtasks seems obvious.
Curriculum learning is an extension to the widely used transfer learning in other Machine Learning areas~\cite{wiering_transfer_2012}.
Its goal is to create a series of source tasks for the agent to train on, such that the acquired knowledge
can be transferred to increase the learning speed and performance on the target task~\cite{narvekar_learning_2018}.
The major obstacle in using curriculum learning is building and choosing the subtasks~\cite{narvekar2016source}.
For this specific game three smaller sub-levels were created of the labyrinth~\cite{fig:unity_levels}.
The main idea is to train the agent on the whole labyrinth without holes first, to ensure that it is able to follow the specified path reliably.
In the following stages more and more holes are introduced, such that only small adjustments to the movements have to be made.
An agent advances to the next subtask, when it is able to solve the current task 100 times.

\begin{figure}[h]
    \centering
    \caption{Four stages of the labyrinth used for curriulum learning.}
    \label{fig:unity_levels}
\end{figure}

To train the agent the Proximal Policy Optimization (PPO)~\cite{schulman_proximal_2017} algorithm is used.
Since it's an on-policy algorithm that aims to find the best balance between exploration and exploitation, it's especially
well suited for this task in a continuous environment.
The alternative ML-Agents offer is is Soft-Actor Critic (SAC)~\cite{haarnoja_soft_2018}.
This approach is not used, as it is better suited for a sparse reward system, higher dimensional action space and heavier or slower
environments, with about 0.1s between steps, as suggested by Unity.

\subsection{Environment}\label{subsec:environment}

\subsubsection{Action Space}
The action space refers to the set of possible actions an agent can take in a given environment.
It is usually represented as a discrete or continuous space, where discrete actions are chosen as distinct elements
out of a finite set, whereas continuous actions are real-valued vectors, and in Unity's case are clipped between $[-1, 1]$.
As the controls of this game are fairly simple, so is the action space of this reinforcement learning task.
The size of the action space is set to two floats, meaning the Python API fills a buffer with two floats ranging between $[-1, 1]$ that will be converted into
the corresponding controls:
\begin{enumerate}
    \item{Rotation on X-Axis}: Rotates the labyrinth on the x-axis by the given amount in the action buffer multiplied with a constant rotation factor of 0.1.
    Depending on the sign of the action the board is either tilted towards positive or negative x.
    \item{Rotation on Z-Axis}: Rotates the labyrinth equivalent on the z-axis.
    Unity uses a left-handed, y-up coordinate system.
\end{enumerate}
Both actions are disconnected, meaning in one step the labyrinth can be tilted both on the x- and z-axis simultaneously.
By using a continuous action space the agent can tilt the game board more precisely, which results in finer control over the marble.

\subsubsection{Observation Space}
Contrary to the action space, the observation space describes the input of the agent, information that the agent receives from the environment.
Is can also be discrete or continuous and is used to inform the decision making process of the agent.
The difficult trade-off is giving the agent enough data about it's surrounding to be able to converge on a good solution and not giving
too much irrelevant data, which slows down the learning process or could lead to suboptimal performances.
Agents in the ML-Agents package distinguish between two types of observations: vectors and images.
Images can be of arbitrary size with either grayscale or color information and vectors can be of any dimension with numerical values.
As mentioned above observations can also be collected off of Sensors like the RayPerceptionSensor, which yields information about
a ray-hit with another game object, like the normalized distance.
The best results were achieved with following set of observations:
\begin{itemize}
    \item{Rotation on X-Axis}: 4 floats represented as a quaternion
    \item{Rotation on Z-Axis}: 4 floats represented as a quaternion
    \item{Normalized Position of the marble}: 3 floats
    \item{Magnitude of Marble Velocity}: 1 float
    \item{Normalized Euclidian Distance to the next Checkpoint}: 1 float
    \item 
\end{itemize}