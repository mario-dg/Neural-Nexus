%! Author = mario
%! Date = 25.01.2023


\section{Basics}\label{sec:basics}
There are several paradigms of ML, including supervised learning, unsupervised learning, and reinforcement learning.
In supervised learning, the model is trained on a labeled dataset, where the correct output is provided for each input.
Unsupervised learning involves training the model on an unlabeled dataset, where the goal is to find patterns or structure in the data.
RL is a type of learning where an agent learns to make decisions by interacting with an environment, and receiving feedback in the form of rewards or penalties.
Sutton and Barto~\cite{sutton_reinforcement_2018} compare this approach to an infant with no explicit teacher, but with a direct connection to it's environment.

\begin{quote}
    ``Exercising this connection produces a wealth of information about
    cause and effect, about the consequences of actions, and about what to do in order to
    achieve goals.``\cite{sutton_reinforcement_2018}
\end{quote}

\subsection{Reinforcement Learning}\label{subsec:reinforcement-learning}
In contrast to, for example supervised learning, where the model tries to learn a function that maps
the input data to the specified targets, in reinforcement learning the agent's goal is to learn a policy.
The policy describes a mapping from states to actions, that maximizes the expected cumulative reward over time.
This process is particularly useful when the desired behavior is not explicitly provided, but can be inferred by the agent
through trial and error.\\
The agent observes the current state of the environment and chooses an action based on its current policy.
The environment then changes and the agent is rewarded or punished based on the action it took.
By utilizing this feedback, the agent updates its policy and enhances its decision-making over time.
Value-based approaches, which learn the value of different states or state-action combinations, and policy-based methods, which directly learn the best policy, are two types of RL algorithms.
There are also model-based techniques, which use a model of the environment to anticipate the outcomes of certain actions.
For this task the policy-based method PPO was chosen.

\subsection{Proximal Policy Optmization (PPO)}\label{subsec:proximal-policy-optmization-(ppo)}
Proximal Policy Optimization (PPO) is a type of algorithm used to optimize a reinforcement learning (RL) model.
It is an improvement on a previous algorithm called Trust Region Policy Optimization (TRPO)~\cite{schulman_proximal_2017}.
PPO's primary notion is to take incremental, -proximal- steps toward refining the policy (the set of behaviors the model does in different scenarios) rather than huge, possibly unreliable changes.
PPO does this by employing a unique loss function (surrogate loss) that promotes the model to make adjustments that are comparable to past actions rather than making large jumps.
This makes the learning process more steady and reduces the likelihood of being stuck in a bad policy.
It also uses the actor-critic method~\cite{konda_onactor-critic_2003}.
In summary, the actor-critic approach is made up of two parts: the actor, who is in charge of selecting actions, and the critic, who is in charge of evaluating the quality of the actions.
The actor is optimized by maximizing the surrogate loss function, while the critic offers feedback by assessing the value of the state-action pairs.
This enables the agent to learn both the policy and the value function at the same time.
The whole process is repeated until the agent solves the the environment.

\subsection{Unity and ML-Agents}\label{subsec:unity-and-ml-agents}
Unity is a popular cross-platform game engine that can be used to create 2D and 3D video games, simulations, and other interactive content.
It was first launched in 2005 and has since evolved to become one of the industry's most extensively utilized game engines~\cite{Haas2014AHO}.


