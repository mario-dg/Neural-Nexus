@article{juliani2020,
    title = {Unity: A general platform for intelligent agents},
    author = {Juliani, Arthur and Berges, Vincent-Pierre and Teng, Ervin and Cohen, Andrew and Harper, Jonathan and Elion, Chris and Goy, Chris and Gao, Yuan and Henry, Hunter and Mattar, Marwan and Lange, Danny},
    journal = {arXiv preprint arXiv:1809.02627},
    year = {2020}
}

@article{narvekar_learning_2018,
    title = {Learning {Curriculum} {Policies} for {Reinforcement} {Learning}},
    copyright = {arXiv.org perpetual, non-exclusive license},
    url = {https://arxiv.org/abs/1812.00285},
    doi = {10.48550/ARXIV.1812.00285},
    abstract = {Curriculum learning in reinforcement learning is a training methodology that seeks to speed up learning of a difficult target task, by first training on a series of simpler tasks and transferring the knowledge acquired to the target task. Automatically choosing a sequence of such tasks (i.e. a curriculum) is an open problem that has been the subject of much recent work in this area. In this paper, we build upon a recent method for curriculum design, which formulates the curriculum sequencing problem as a Markov Decision Process. We extend this model to handle multiple transfer learning algorithms, and show for the first time that a curriculum policy over this MDP can be learned from experience. We explore various representations that make this possible, and evaluate our approach by learning curriculum policies for multiple agents in two different domains. The results show that our method produces curricula that can train agents to perform on a target task as fast or faster than existing methods.},
    urldate = {2023-01-25},
    author = {Narvekar, Sanmit and Stone, Peter},
    year = {2018},
    note = {Publisher: arXiv
Version Number: 1},
    keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML)},
}

@book{sutton_reinforcement_2018,
    address = {Cambridge, Massachusetts},
    edition = {Second edition},
    series = {Adaptive computation and machine learning series},
    title = {Reinforcement learning: an introduction},
    isbn = {978-0-262-03924-6},
    shorttitle = {Reinforcement learning},
    abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
    publisher = {The MIT Press},
    author = {Sutton, Richard S. and Barto, Andrew G.},
    year = {2018},
    keywords = {Reinforcement learning},
}

@article{haarnoja_soft_2018,
    title = {Soft {Actor}-{Critic}: {Off}-{Policy} {Maximum} {Entropy} {Deep} {Reinforcement} {Learning} with a {Stochastic} {Actor}},
    copyright = {arXiv.org perpetual, non-exclusive license},
    shorttitle = {Soft {Actor}-{Critic}},
    url = {https://arxiv.org/abs/1801.01290},
    doi = {10.48550/ARXIV.1801.01290},
    abstract = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
    urldate = {2023-01-31},
    author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
    year = {2018},
    note = {Publisher: arXiv
Version Number: 2},
    keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML)},
}

@incollection{wiering_transfer_2012,
    address = {Berlin, Heidelberg},
    title = {Transfer in {Reinforcement} {Learning}: {A} {Framework} and a {Survey}},
    volume = {12},
    isbn = {978-3-642-27644-6 978-3-642-27645-3},
    shorttitle = {Transfer in {Reinforcement} {Learning}},
    url = {http://link.springer.com/10.1007/978-3-642-27645-3_5},
    urldate = {2023-01-31},
    booktitle = {Reinforcement {Learning}},
    publisher = {Springer Berlin Heidelberg},
    author = {Lazaric, Alessandro},
    editor = {Wiering, Marco and van Otterlo, Martijn},
    year = {2012},
    doi = {10.1007/978-3-642-27645-3_5},
    note = {Series Title: Adaptation, Learning, and Optimization},
    pages = {143--173},
}

@article{konda_onactor-critic_2003,
    title = {{OnActor}-{Critic} {Algorithms}},
    volume = {42},
    issn = {0363-0129, 1095-7138},
    url = {http://epubs.siam.org/doi/10.1137/S0363012901385691},
    doi = {10.1137/S0363012901385691},
    language = {en},
    number = {4},
    urldate = {2023-01-25},
    journal = {SIAM Journal on Control and Optimization},
    author = {Konda, Vijay R. and Tsitsiklis, John N.},
    month = jan,
    year = {2003},
    pages = {1143--1166},
}

@article{schulman_proximal_2017,
    title = {Proximal {Policy} {Optimization} {Algorithms}},
    url = {http://arxiv.org/abs/1707.06347},
    abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
    urldate = {2023-01-25},
    publisher = {arXiv},
    author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
    month = aug,
    year = {2017},
    note = {arXiv:1707.06347 [cs]},
    keywords = {Computer Science - Machine Learning},
}
@inproceedings{Haas2014AHO,
    title={A History of the Unity Game Engine},
    author={John K Haas},
    year={2014}
}

@MISC{wooden_board,
    author = {{unknown}},
    title = {Wooden Labyrinth Puzzle Maze Game 36 Pass Balance Board Educational Skill Improvement Toys for Kids},
    year = {unknown},
    note = {[Online; accessed January 30, 2023]},
    url = {https://shopee.com.my/Wooden-Labyrinth-Puzzle-Maze-Game-36-Pass-Balance-Board-Educational-Skill-Improvement-Toys-for-Kids-i.82073268.2508203260}
}
@Manual{blender,
title = {Blender - a 3D modelling and rendering package},
author = {Blender Online Community},
organization = {Blender Foundation},
address = {Stichting Blender Foundation, Amsterdam},
year = {2018},
url = {http://www.blender.org},
}

@inproceedings{narvekar2016source,
    title={Source task creation for curriculum learning},
    author={Narvekar, Sanmit and Sinapov, Jivko and Leonetti, Matteo and Stone, Peter},
    booktitle={Proceedings of the 2016 international conference on autonomous agents \& multiagent systems},
    pages={566--574},
    year={2016}
}
