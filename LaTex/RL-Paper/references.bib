@article{juliani2020,
    title = {Unity: A general platform for intelligent agents},
    author = {Juliani, Arthur and Berges, Vincent-Pierre and Teng, Ervin and Cohen, Andrew and Harper, Jonathan and Elion, Chris and Goy, Chris and Gao, Yuan and Henry, Hunter and Mattar, Marwan and Lange, Danny},
    journal = {arXiv preprint arXiv:1809.02627},
    year = {2020}
}

@article{narvekar_learning_2018,
    title = {Learning {Curriculum} {Policies} for {Reinforcement} {Learning}},
    copyright = {arXiv.org perpetual, non-exclusive license},
    url = {https://arxiv.org/abs/1812.00285},
    doi = {10.48550/ARXIV.1812.00285},
    abstract = {Curriculum learning in reinforcement learning is a training methodology that seeks to speed up learning of a difficult target task, by first training on a series of simpler tasks and transferring the knowledge acquired to the target task. Automatically choosing a sequence of such tasks (i.e. a curriculum) is an open problem that has been the subject of much recent work in this area. In this paper, we build upon a recent method for curriculum design, which formulates the curriculum sequencing problem as a Markov Decision Process. We extend this model to handle multiple transfer learning algorithms, and show for the first time that a curriculum policy over this MDP can be learned from experience. We explore various representations that make this possible, and evaluate our approach by learning curriculum policies for multiple agents in two different domains. The results show that our method produces curricula that can train agents to perform on a target task as fast or faster than existing methods.},
    urldate = {2023-01-25},
    author = {Narvekar, Sanmit and Stone, Peter},
    year = {2018},
    note = {Publisher: arXiv
Version Number: 1},
    keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML)},
}

@book{sutton_reinforcement_2018,
    address = {Cambridge, Massachusetts},
    edition = {Second edition},
    series = {Adaptive computation and machine learning series},
    title = {Reinforcement learning: an introduction},
    isbn = {978-0-262-03924-6},
    shorttitle = {Reinforcement learning},
    abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
    publisher = {The MIT Press},
    author = {Sutton, Richard S. and Barto, Andrew G.},
    year = {2018},
    keywords = {Reinforcement learning},
}

@article{konda_onactor-critic_2003,
    title = {{OnActor}-{Critic} {Algorithms}},
    volume = {42},
    issn = {0363-0129, 1095-7138},
    url = {http://epubs.siam.org/doi/10.1137/S0363012901385691},
    doi = {10.1137/S0363012901385691},
    language = {en},
    number = {4},
    urldate = {2023-01-25},
    journal = {SIAM Journal on Control and Optimization},
    author = {Konda, Vijay R. and Tsitsiklis, John N.},
    month = jan,
    year = {2003},
    pages = {1143--1166},
}

@article{schulman_proximal_2017,
    title = {Proximal {Policy} {Optimization} {Algorithms}},
    url = {http://arxiv.org/abs/1707.06347},
    abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
    urldate = {2023-01-25},
    publisher = {arXiv},
    author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
    month = aug,
    year = {2017},
    note = {arXiv:1707.06347 [cs]},
    keywords = {Computer Science - Machine Learning},
}
@inproceedings{Haas2014AHO,
    title={A History of the Unity Game Engine},
    author={John K Haas},
    year={2014}
}
