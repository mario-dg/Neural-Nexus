@article{juliani2020,
    title = {Unity: A general platform for intelligent agents},
    author = {Juliani, Arthur and Berges, Vincent-Pierre and Teng, Ervin and Cohen, Andrew and Harper, Jonathan and Elion, Chris and Goy, Chris and Gao, Yuan and Henry, Hunter and Mattar, Marwan and Lange, Danny},
    journal = {arXiv preprint arXiv:1809.02627},
    year = {2020}
}

@article{narvekar_learning_2018,
    title = {Learning {Curriculum} {Policies} for {Reinforcement} {Learning}},
    copyright = {arXiv.org perpetual, non-exclusive license},
    url = {https://arxiv.org/abs/1812.00285},
    doi = {10.48550/ARXIV.1812.00285},
    abstract = {Curriculum learning in reinforcement learning is a training methodology that seeks to speed up learning of a difficult target task, by first training on a series of simpler tasks and transferring the knowledge acquired to the target task. Automatically choosing a sequence of such tasks (i.e. a curriculum) is an open problem that has been the subject of much recent work in this area. In this paper, we build upon a recent method for curriculum design, which formulates the curriculum sequencing problem as a Markov Decision Process. We extend this model to handle multiple transfer learning algorithms, and show for the first time that a curriculum policy over this MDP can be learned from experience. We explore various representations that make this possible, and evaluate our approach by learning curriculum policies for multiple agents in two different domains. The results show that our method produces curricula that can train agents to perform on a target task as fast or faster than existing methods.},
    urldate = {2023-01-25},
    author = {Narvekar, Sanmit and Stone, Peter},
    year = {2018},
    note = {Publisher: arXiv
Version Number: 1},
    keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML)},
}

@book{sutton_reinforcement_2018,
    address = {Cambridge, Massachusetts},
    edition = {Second edition},
    series = {Adaptive computation and machine learning series},
    title = {Reinforcement learning: an introduction},
    isbn = {978-0-262-03924-6},
    shorttitle = {Reinforcement learning},
    abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
    publisher = {The MIT Press},
    author = {Sutton, Richard S. and Barto, Andrew G.},
    year = {2018},
    keywords = {Reinforcement learning},
}

@article{schulman_high-dimensional_2015,
    title = {High-{Dimensional} {Continuous} {Control} {Using} {Generalized} {Advantage} {Estimation}},
    copyright = {arXiv.org perpetual, non-exclusive license},
    url = {https://arxiv.org/abs/1506.02438},
    doi = {10.48550/ARXIV.1506.02438},
    abstract = {Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.},
    urldate = {2023-02-06},
    author = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
    year = {2015},
    note = {Publisher: arXiv
Version Number: 6},
    keywords = {FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, Machine Learning (cs.LG), Robotics (cs.RO), Systems and Control (eess.SY)},
}

@article{sutton_introduction_1992,
    title = {Introduction: {The} challenge of reinforcement learning},
    volume = {8},
    issn = {0885-6125, 1573-0565},
    shorttitle = {Introduction},
    url = {http://link.springer.com/10.1007/BF00992695},
    doi = {10.1007/BF00992695},
    language = {en},
    number = {3-4},
    urldate = {2023-01-31},
    journal = {Machine Learning},
    author = {Sutton, Richard S.},
    month = may,
    year = {1992},
    pages = {225--227},
}

@article{song_observational_2019,
    title = {Observational {Overfitting} in {Reinforcement} {Learning}},
    copyright = {arXiv.org perpetual, non-exclusive license},
    url = {https://arxiv.org/abs/1912.02975},
    doi = {10.48550/ARXIV.1912.02975},
    abstract = {A major component of overfitting in model-free reinforcement learning (RL) involves the case where the agent may mistakenly correlate reward with certain spurious features from the observations generated by the Markov Decision Process (MDP). We provide a general framework for analyzing this scenario, which we use to design multiple synthetic benchmarks from only modifying the observation space of an MDP. When an agent overfits to different observation spaces even if the underlying MDP dynamics is fixed, we term this observational overfitting. Our experiments expose intriguing properties especially with regards to implicit regularization, and also corroborate results from previous works in RL generalization and supervised learning (SL).},
    urldate = {2023-01-31},
    author = {Song, Xingyou and Jiang, Yiding and Tu, Stephen and Du, Yilun and Neyshabur, Behnam},
    year = {2019},
    note = {Publisher: arXiv, Version Number: 2},
    keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML)},
}

@article{haarnoja_soft_2018,
    title = {Soft {Actor}-{Critic}: {Off}-{Policy} {Maximum} {Entropy} {Deep} {Reinforcement} {Learning} with a {Stochastic} {Actor}},
    copyright = {arXiv.org perpetual, non-exclusive license},
    shorttitle = {Soft {Actor}-{Critic}},
    url = {https://arxiv.org/abs/1801.01290},
    doi = {10.48550/ARXIV.1801.01290},
    abstract = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
    urldate = {2023-01-31},
    author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
    year = {2018},
    note = {Publisher: arXiv
Version Number: 2},
    keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML)},
}

@incollection{wiering_transfer_2012,
    address = {Berlin, Heidelberg},
    title = {Transfer in {Reinforcement} {Learning}: {A} {Framework} and a {Survey}},
    volume = {12},
    isbn = {978-3-642-27644-6 978-3-642-27645-3},
    shorttitle = {Transfer in {Reinforcement} {Learning}},
    url = {http://link.springer.com/10.1007/978-3-642-27645-3_5},
    urldate = {2023-01-31},
    booktitle = {Reinforcement {Learning}},
    publisher = {Springer Berlin Heidelberg},
    author = {Lazaric, Alessandro},
    editor = {Wiering, Marco and van Otterlo, Martijn},
    year = {2012},
    doi = {10.1007/978-3-642-27645-3_5},
    note = {Series Title: Adaptation, Learning, and Optimization},
    pages = {143--173},
}

@article{schulman_proximal_2017,
    title = {Proximal {Policy} {Optimization} {Algorithms}},
    url = {http://arxiv.org/abs/1707.06347},
    abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
    urldate = {2023-01-25},
    publisher = {arXiv},
    author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
    month = aug,
    year = {2017},
    note = {arXiv:1707.06347 [cs]},
    keywords = {Computer Science - Machine Learning},
}
@inproceedings{Haas2014AHO,
    title={A History of the Unity Game Engine},
    author={John K Haas},
    year={2014}
}

@MISC{wooden_board,
    author = {{unknown}},
    title = {Wooden Labyrinth Puzzle Maze Game 36 Pass Balance Board Educational Skill Improvement Toys for Kids},
    year = {unknown},
    note = {[Online; accessed January 30, 2023]},
    url = {https://shopee.com.my/Wooden-Labyrinth-Puzzle-Maze-Game-36-Pass-Balance-Board-Educational-Skill-Improvement-Toys-for-Kids-i.82073268.2508203260}
}
@Manual{blender,
title = {Blender - a 3D modelling and rendering package},
author = {Blender Online Community},
organization = {Blender Foundation},
address = {Stichting Blender Foundation, Amsterdam},
year = {2018},
url = {http://www.blender.org},
}

@inproceedings{narvekar2016source,
    title={Source task creation for curriculum learning},
    author={Narvekar, Sanmit and Sinapov, Jivko and Leonetti, Matteo and Stone, Peter},
    booktitle={Proceedings of the 2016 international conference on autonomous agents \& multiagent systems},
    pages={566--574},
    year={2016}
}

@InProceedings{parallel-training,
    title = 	 {Learning to Walk in Minutes Using Massively Parallel Deep Reinforcement Learning},
    author =       {Rudin, Nikita and Hoeller, David and Reist, Philipp and Hutter, Marco},
    booktitle = 	 {Proceedings of the 5th Conference on Robot Learning},
    pages = 	 {91--100},
    year = 	 {2022},
    editor = 	 {Faust, Aleksandra and Hsu, David and Neumann, Gerhard},
    volume = 	 {164},
    series = 	 {Proceedings of Machine Learning Research},
    month = 	 {08--11 Nov},
    publisher =    {PMLR},
    pdf = 	 {https://proceedings.mlr.press/v164/rudin22a/rudin22a.pdf},
    url = 	 {https://proceedings.mlr.press/v164/rudin22a.html},
    abstract = 	 {In this work, we present and study a training set-up that achieves fast policy generation for real-world robotic tasks by using massive parallelism on a single workstation GPU. We analyze and discuss the impact of different training algorithm components in the massively parallel regime on the final policy performance and training times. In addition, we present a novel game-inspired curriculum that is well suited for training with thousands of simulated robots in parallel. We evaluate the approach by training the quadrupedal robot ANYmal to walk on challenging terrain. The parallel approach allows training policies for flat terrain in under four minutes, and in twenty minutes for uneven terrain. This represents a speedup of multiple orders of magnitude compared to previous work. Finally, we transfer the policies to the real robot to validate the approach. We open-source our training code to help accelerate further research in the field of learned legged locomotion: https://leggedrobotics.github.io/legged_gym/.}
}

